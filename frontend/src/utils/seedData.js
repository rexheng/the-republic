// Pre-fetched landmark AI/ML papers with citation relationships
// Guarantees the demo always works, even offline

const SEED_PAPERS = [
  {
    id: 'vaswani2017',
    paperId: 'vaswani2017',
    title: 'Attention Is All You Need',
    authors: ['Ashish Vaswani', 'Noam Shazeer', 'Niki Parmar', 'Jakob Uszkoreit', 'Llion Jones', 'Aidan N. Gomez', 'Lukasz Kaiser', 'Illia Polosukhin'],
    year: 2017,
    citationCount: 120000,
    abstract: 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.1706.03762',
    source: 'seed',
    githubRepo: 'tensorflow/tensor2tensor',
  },
  {
    id: 'devlin2019',
    paperId: 'devlin2019',
    title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',
    authors: ['Jacob Devlin', 'Ming-Wei Chang', 'Kenton Lee', 'Kristina Toutanova'],
    year: 2019,
    citationCount: 85000,
    abstract: 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.1810.04805',
    source: 'seed',
    githubRepo: 'google-research/bert',
  },
  {
    id: 'brown2020',
    paperId: 'brown2020',
    title: 'Language Models are Few-Shot Learners',
    authors: ['Tom B. Brown', 'Benjamin Mann', 'Nick Ryder', 'Melanie Subbiah'],
    year: 2020,
    citationCount: 35000,
    abstract: 'We show that scaling up language models greatly improves task-agnostic, few-shot performance. GPT-3 achieves strong performance on many NLP benchmarks.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.2005.14165',
    source: 'seed',
  },
  {
    id: 'openai2023',
    paperId: 'openai2023',
    title: 'GPT-4 Technical Report',
    authors: ['OpenAI'],
    year: 2023,
    citationCount: 12000,
    abstract: 'We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.2303.08774',
    source: 'seed',
  },
  {
    id: 'he2016',
    paperId: 'he2016',
    title: 'Deep Residual Learning for Image Recognition',
    authors: ['Kaiming He', 'Xiangyu Zhang', 'Shaoqing Ren', 'Jian Sun'],
    year: 2016,
    citationCount: 190000,
    abstract: 'We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.1109/CVPR.2016.90',
    source: 'seed',
    githubRepo: 'KaimingHe/deep-residual-networks',
  },
  {
    id: 'goodfellow2014',
    paperId: 'goodfellow2014',
    title: 'Generative Adversarial Networks',
    authors: ['Ian J. Goodfellow', 'Jean Pouget-Abadie', 'Mehdi Mirza', 'Bing Xu', 'David Warde-Farley', 'Sherjil Ozair', 'Aaron Courville', 'Yoshua Bengio'],
    year: 2014,
    citationCount: 65000,
    abstract: 'We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.1406.2661',
    source: 'seed',
    githubRepo: 'goodfeli/adversarial',
  },
  {
    id: 'krizhevsky2012',
    paperId: 'krizhevsky2012',
    title: 'ImageNet Classification with Deep Convolutional Neural Networks',
    authors: ['Alex Krizhevsky', 'Ilya Sutskever', 'Geoffrey E. Hinton'],
    year: 2012,
    citationCount: 130000,
    abstract: 'We trained a large, deep convolutional neural network to classify the 1.2 million images in the ImageNet LSVRC-2010 contest into the 1000 different classes.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.1145/3065386',
    source: 'seed',
  },
  {
    id: 'hochreiter1997',
    paperId: 'hochreiter1997',
    title: 'Long Short-Term Memory',
    authors: ['Sepp Hochreiter', 'Jurgen Schmidhuber'],
    year: 1997,
    citationCount: 95000,
    abstract: 'Learning to store information over extended time intervals by recurrent backpropagation takes a very long time. We introduce Long Short-Term Memory.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.1162/neco.1997.9.8.1735',
    source: 'seed',
  },
  {
    id: 'kingma2015',
    paperId: 'kingma2015',
    title: 'Adam: A Method for Stochastic Optimization',
    authors: ['Diederik P. Kingma', 'Jimmy Ba'],
    year: 2015,
    citationCount: 175000,
    abstract: 'We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.1412.6980',
    source: 'seed',
  },
  {
    id: 'lecun1998',
    paperId: 'lecun1998',
    title: 'Gradient-Based Learning Applied to Document Recognition',
    authors: ['Yann LeCun', 'Leon Bottou', 'Yoshua Bengio', 'Patrick Haffner'],
    year: 1998,
    citationCount: 50000,
    abstract: 'Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.1109/5.726791',
    source: 'seed',
  },
  {
    id: 'radford2021',
    paperId: 'radford2021',
    title: 'Learning Transferable Visual Models From Natural Language Supervision',
    authors: ['Alec Radford', 'Jong Wook Kim', 'Chris Hallacy', 'Aditya Ramesh', 'Gabriel Goh', 'Sandhini Agarwal'],
    year: 2021,
    citationCount: 18000,
    abstract: 'We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations. CLIP.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.2103.00020',
    source: 'seed',
    githubRepo: 'openai/CLIP',
  },
  {
    id: 'rombach2022',
    paperId: 'rombach2022',
    title: 'High-Resolution Image Synthesis with Latent Diffusion Models',
    authors: ['Robin Rombach', 'Andreas Blattmann', 'Dominik Lorenz', 'Patrick Esser', 'Bjorn Ommer'],
    year: 2022,
    citationCount: 14000,
    abstract: 'We apply diffusion models in the latent space of powerful pretrained autoencoders. Stable Diffusion.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.1109/CVPR52688.2022.10674',
    source: 'seed',
    githubRepo: 'CompVis/stable-diffusion',
  },
  {
    id: 'ho2020',
    paperId: 'ho2020',
    title: 'Denoising Diffusion Probabilistic Models',
    authors: ['Jonathan Ho', 'Ajay Jain', 'Pieter Abbeel'],
    year: 2020,
    citationCount: 12000,
    abstract: 'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.2006.11239',
    source: 'seed',
    githubRepo: 'hojonathanho/diffusion',
  },
  {
    id: 'dosovitskiy2021',
    paperId: 'dosovitskiy2021',
    title: 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale',
    authors: ['Alexey Dosovitskiy', 'Lucas Beyer', 'Alexander Kolesnikov', 'Dirk Weissenborn'],
    year: 2021,
    citationCount: 30000,
    abstract: 'We show that a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. Vision Transformer (ViT).',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.2010.11929',
    source: 'seed',
    githubRepo: 'google-research/vision_transformer',
  },
  {
    id: 'silver2016',
    paperId: 'silver2016',
    title: 'Mastering the Game of Go with Deep Neural Networks and Tree Search',
    authors: ['David Silver', 'Aja Huang', 'Chris J. Maddison', 'Arthur Guez', 'Laurent Sifre'],
    year: 2016,
    citationCount: 18000,
    abstract: 'We introduce a new approach to computer Go that uses value networks to evaluate board positions and policy networks to select moves. AlphaGo.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.1038/nature16961',
    source: 'seed',
    githubRepo: 'Rochester-NRT/AlphaGo',
  },
  {
    id: 'mnih2015',
    paperId: 'mnih2015',
    title: 'Human-level Control through Deep Reinforcement Learning',
    authors: ['Volodymyr Mnih', 'Koray Kavukcuoglu', 'David Silver', 'Andrei A. Rusu'],
    year: 2015,
    citationCount: 28000,
    abstract: 'We demonstrate that deep reinforcement learning can achieve human-level performance across a set of 49 Atari 2600 games. DQN.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.1038/nature14236',
    source: 'seed',
    githubRepo: 'deepmind/dqn',
  },
  {
    id: 'schulman2017',
    paperId: 'schulman2017',
    title: 'Proximal Policy Optimization Algorithms',
    authors: ['John Schulman', 'Filip Wolski', 'Prafulla Dhariwal', 'Alec Radford', 'Oleg Klimov'],
    year: 2017,
    citationCount: 15000,
    abstract: 'We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data and optimizing a surrogate objective function. PPO.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.1707.06347',
    source: 'seed',
    githubRepo: 'openai/baselines',
  },
  {
    id: 'kipf2017',
    paperId: 'kipf2017',
    title: 'Semi-Supervised Classification with Graph Convolutional Networks',
    authors: ['Thomas N. Kipf', 'Max Welling'],
    year: 2017,
    citationCount: 25000,
    abstract: 'We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. GCN.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.1609.02907',
    source: 'seed',
    githubRepo: 'tkipf/gcn',
  },
  {
    id: 'hamilton2017',
    paperId: 'hamilton2017',
    title: 'Inductive Representation Learning on Large Graphs',
    authors: ['William L. Hamilton', 'Rex Ying', 'Jure Leskovec'],
    year: 2017,
    citationCount: 12000,
    abstract: 'We present GraphSAGE, a general inductive framework that leverages node feature information to efficiently generate node embeddings for previously unseen data.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.1706.02216',
    source: 'seed',
  },
  {
    id: 'velickovic2018',
    paperId: 'velickovic2018',
    title: 'Graph Attention Networks',
    authors: ['Petar Velickovic', 'Guillem Cucurull', 'Arantxa Casanova', 'Adriana Romero', 'Pietro Lio', 'Yoshua Bengio'],
    year: 2018,
    citationCount: 14000,
    abstract: 'We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.1710.10903',
    source: 'seed',
  },
  {
    id: 'raffel2020',
    paperId: 'raffel2020',
    title: 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer',
    authors: ['Colin Raffel', 'Noam Shazeer', 'Adam Roberts', 'Katherine Lee', 'Sharan Narang'],
    year: 2020,
    citationCount: 18000,
    abstract: 'We introduce a unified framework that converts all text-based language problems into a text-to-text format. T5.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.1910.10683',
    source: 'seed',
  },
  {
    id: 'touvron2023',
    paperId: 'touvron2023',
    title: 'LLaMA: Open and Efficient Foundation Language Models',
    authors: ['Hugo Touvron', 'Thibaut Lavril', 'Gautier Izacard', 'Xavier Martinet'],
    year: 2023,
    citationCount: 9000,
    abstract: 'We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters trained on publicly available datasets exclusively.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.2302.13971',
    source: 'seed',
    githubRepo: 'facebookresearch/llama',
  },
  {
    id: 'ouyang2022',
    paperId: 'ouyang2022',
    title: 'Training Language Models to Follow Instructions with Human Feedback',
    authors: ['Long Ouyang', 'Jeff Wu', 'Xu Jiang', 'Diogo Almeida'],
    year: 2022,
    citationCount: 11000,
    abstract: 'We show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. InstructGPT / RLHF.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.2203.02155',
    source: 'seed',
  },
  {
    id: 'wei2022',
    paperId: 'wei2022',
    title: 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models',
    authors: ['Jason Wei', 'Xuezhi Wang', 'Dale Schuurmans', 'Maarten Bosma', 'Brian Ichter'],
    year: 2022,
    citationCount: 8000,
    abstract: 'We explore how generating a chain of thought — a series of intermediate reasoning steps — significantly improves the ability of large language models to perform complex reasoning.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.2201.11903',
    source: 'seed',
  },
  {
    id: 'liu2019',
    paperId: 'liu2019',
    title: 'RoBERTa: A Robustly Optimized BERT Pretraining Approach',
    authors: ['Yinhan Liu', 'Myle Ott', 'Naman Goyal', 'Jingfei Du'],
    year: 2019,
    citationCount: 20000,
    abstract: 'We find that BERT was significantly undertrained and propose RoBERTa, an improved recipe for training BERT models.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.1907.11692',
    source: 'seed',
  },
  {
    id: 'radford2019',
    paperId: 'radford2019',
    title: 'Language Models are Unsupervised Multitask Learners',
    authors: ['Alec Radford', 'Jeff Wu', 'Rewon Child', 'David Luan', 'Dario Amodei', 'Ilya Sutskever'],
    year: 2019,
    citationCount: 22000,
    abstract: 'We demonstrate that language models begin to learn NLP tasks without any explicit supervision when trained on a new dataset of millions of webpages. GPT-2.',
    fieldsOfStudy: ['Computer Science'],
    doi: null,
    source: 'seed',
    githubRepo: 'openai/gpt-2',
  },
  {
    id: 'mikolov2013',
    paperId: 'mikolov2013',
    title: 'Efficient Estimation of Word Representations in Vector Space',
    authors: ['Tomas Mikolov', 'Kai Chen', 'Greg Corrado', 'Jeffrey Dean'],
    year: 2013,
    citationCount: 40000,
    abstract: 'We propose two novel model architectures for computing continuous vector representations of words from very large data sets. Word2Vec.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.1301.3781',
    source: 'seed',
  },
  {
    id: 'simonyan2015',
    paperId: 'simonyan2015',
    title: 'Very Deep Convolutional Networks for Large-Scale Image Recognition',
    authors: ['Karen Simonyan', 'Andrew Zisserman'],
    year: 2015,
    citationCount: 100000,
    abstract: 'We investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. VGGNet.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.1409.1556',
    source: 'seed',
  },
  {
    id: 'szegedy2015',
    paperId: 'szegedy2015',
    title: 'Going Deeper with Convolutions',
    authors: ['Christian Szegedy', 'Wei Liu', 'Yangqing Jia', 'Pierre Sermanet', 'Scott Reed'],
    year: 2015,
    citationCount: 45000,
    abstract: 'We propose a deep convolutional neural network architecture codenamed Inception that achieves new state of the art for classification and detection. GoogLeNet.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.1109/CVPR.2015.7298594',
    source: 'seed',
  },
  {
    id: 'ronneberger2015',
    paperId: 'ronneberger2015',
    title: 'U-Net: Convolutional Networks for Biomedical Image Segmentation',
    authors: ['Olaf Ronneberger', 'Philipp Fischer', 'Thomas Brox'],
    year: 2015,
    citationCount: 70000,
    abstract: 'We present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. U-Net.',
    fieldsOfStudy: ['Computer Science', 'Medicine'],
    doi: '10.1007/978-3-319-24574-4_28',
    source: 'seed',
  },
  {
    id: 'redmon2016',
    paperId: 'redmon2016',
    title: 'You Only Look Once: Unified, Real-Time Object Detection',
    authors: ['Joseph Redmon', 'Santosh Divvala', 'Ross Girshick', 'Ali Farhadi'],
    year: 2016,
    citationCount: 40000,
    abstract: 'We frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. YOLO.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.1109/CVPR.2016.91',
    source: 'seed',
  },
  {
    id: 'ren2015',
    paperId: 'ren2015',
    title: 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks',
    authors: ['Shaoqing Ren', 'Kaiming He', 'Ross Girshick', 'Jian Sun'],
    year: 2015,
    citationCount: 55000,
    abstract: 'We introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network. Faster R-CNN.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.1506.01497',
    source: 'seed',
  },
  {
    id: 'srivastava2014',
    paperId: 'srivastava2014',
    title: 'Dropout: A Simple Way to Prevent Neural Networks from Overfitting',
    authors: ['Nitish Srivastava', 'Geoffrey Hinton', 'Alex Krizhevsky', 'Ilya Sutskever', 'Ruslan Salakhutdinov'],
    year: 2014,
    citationCount: 45000,
    abstract: 'We propose dropout, a technique for addressing overfitting. The key idea is to randomly drop units from the neural network during training.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.5555/2627435.2670313',
    source: 'seed',
  },
  {
    id: 'ioffe2015',
    paperId: 'ioffe2015',
    title: 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift',
    authors: ['Sergey Ioffe', 'Christian Szegedy'],
    year: 2015,
    citationCount: 55000,
    abstract: 'Training deep neural networks is complicated by the fact that the distribution of each layer\'s inputs changes during training. We call this internal covariate shift and propose Batch Normalization.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.1502.03167',
    source: 'seed',
  },
  {
    id: 'hinton2006',
    paperId: 'hinton2006',
    title: 'Reducing the Dimensionality of Data with Neural Networks',
    authors: ['Geoffrey E. Hinton', 'Ruslan R. Salakhutdinov'],
    year: 2006,
    citationCount: 20000,
    abstract: 'We describe an effective way of initializing the weights in a deep network using a generative model with layers of binary or real-valued latent variables. Autoencoders.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.1126/science.1127647',
    source: 'seed',
  },
  {
    id: 'kingma2014',
    paperId: 'kingma2014',
    title: 'Auto-Encoding Variational Bayes',
    authors: ['Diederik P. Kingma', 'Max Welling'],
    year: 2014,
    citationCount: 30000,
    abstract: 'We introduce a stochastic variational inference and learning algorithm that scales to large datasets. VAE.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.1312.6114',
    source: 'seed',
    githubRepo: 'dpkingma/vae_ssl',
  },
  {
    id: 'bahdanau2015',
    paperId: 'bahdanau2015',
    title: 'Neural Machine Translation by Jointly Learning to Align and Translate',
    authors: ['Dzmitry Bahdanau', 'Kyunghyun Cho', 'Yoshua Bengio'],
    year: 2015,
    citationCount: 35000,
    abstract: 'We conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of encoder-decoder architecture and propose to allow a model to automatically search for parts of a source sentence. Attention mechanism.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.1409.0473',
    source: 'seed',
  },
  {
    id: 'sutskever2014',
    paperId: 'sutskever2014',
    title: 'Sequence to Sequence Learning with Neural Networks',
    authors: ['Ilya Sutskever', 'Oriol Vinyals', 'Quoc V. Le'],
    year: 2014,
    citationCount: 25000,
    abstract: 'We present a general end-to-end approach to sequence learning using a multilayered LSTM to map the input sequence to a vector of a fixed dimensionality. Seq2Seq.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.1409.3215',
    source: 'seed',
  },
  {
    id: 'ramesh2022',
    paperId: 'ramesh2022',
    title: 'Hierarchical Text-Conditional Image Generation with CLIP Latents',
    authors: ['Aditya Ramesh', 'Prafulla Dhariwal', 'Alex Nichol', 'Casey Chu', 'Mark Chen'],
    year: 2022,
    citationCount: 6000,
    abstract: 'We propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. DALL-E 2.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.2204.06125',
    source: 'seed',
  },
  {
    id: 'chowdhery2022',
    paperId: 'chowdhery2022',
    title: 'PaLM: Scaling Language Modeling with Pathways',
    authors: ['Aakanksha Chowdhery', 'Sharan Narang', 'Jacob Devlin', 'Maarten Bosma'],
    year: 2022,
    citationCount: 7000,
    abstract: 'We trained a 540-billion parameter dense decoder-only Transformer model using Pathways, achieving breakthrough performance on reasoning tasks. PaLM.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.2204.02311',
    source: 'seed',
  },
  {
    id: 'bubeck2023',
    paperId: 'bubeck2023',
    title: 'Sparks of Artificial General Intelligence: Early experiments with GPT-4',
    authors: ['Sebastien Bubeck', 'Varun Chandrasekaran', 'Ronen Eldan', 'Johannes Gehrke'],
    year: 2023,
    citationCount: 4000,
    abstract: 'We demonstrate that GPT-4 attains a form of general intelligence, showing human-level performance across a wide range of domains.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.2303.12712',
    source: 'seed',
  },
  {
    id: 'kaplan2020',
    paperId: 'kaplan2020',
    title: 'Scaling Laws for Neural Language Models',
    authors: ['Jared Kaplan', 'Sam McCandlish', 'Tom Henighan', 'Tom B. Brown'],
    year: 2020,
    citationCount: 5000,
    abstract: 'We study empirical scaling laws for language model performance on the cross-entropy loss. Performance depends strongly on scale and weakly on model shape.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.2001.08361',
    source: 'seed',
  },
  {
    id: 'lewis2020',
    paperId: 'lewis2020',
    title: 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks',
    authors: ['Patrick Lewis', 'Ethan Perez', 'Aleksandra Piktus', 'Fabio Petroni'],
    year: 2020,
    citationCount: 5000,
    abstract: 'We build RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia. RAG.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.2005.11401',
    source: 'seed',
  },
  {
    id: 'zoph2017',
    paperId: 'zoph2017',
    title: 'Neural Architecture Search with Reinforcement Learning',
    authors: ['Barret Zoph', 'Quoc V. Le'],
    year: 2017,
    citationCount: 8000,
    abstract: 'We use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning. NAS.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.1611.01578',
    source: 'seed',
  },
  {
    id: 'hendrycks2016',
    paperId: 'hendrycks2016',
    title: 'Gaussian Error Linear Units (GELUs)',
    authors: ['Dan Hendrycks', 'Kevin Gimpel'],
    year: 2016,
    citationCount: 10000,
    abstract: 'We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function used in GPT, BERT, and most modern transformers.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.1606.08415',
    source: 'seed',
  },
  {
    id: 'loshchilov2019',
    paperId: 'loshchilov2019',
    title: 'Decoupled Weight Decay Regularization',
    authors: ['Ilya Loshchilov', 'Frank Hutter'],
    year: 2019,
    citationCount: 18000,
    abstract: 'We propose AdamW, fixing the weight decay in Adam by decoupling weight decay from the optimization steps.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.1711.05101',
    source: 'seed',
  },
  {
    id: 'silver2017',
    paperId: 'silver2017',
    title: 'Mastering the Game of Go without Human Knowledge',
    authors: ['David Silver', 'Julian Schrittwieser', 'Karen Simonyan', 'Ioannis Antonoglou'],
    year: 2017,
    citationCount: 13000,
    abstract: 'We introduce AlphaGo Zero, which is trained solely by self-play reinforcement learning, starting from random play, without human data or guidance.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.1038/nature24270',
    source: 'seed',
  },
  {
    id: 'xu2015',
    paperId: 'xu2015',
    title: 'Show, Attend and Tell: Neural Image Caption Generation with Visual Attention',
    authors: ['Kelvin Xu', 'Jimmy Ba', 'Ryan Kiros', 'Kyunghyun Cho', 'Aaron Courville', 'Yoshua Bengio'],
    year: 2015,
    citationCount: 16000,
    abstract: 'We introduce an attention based model that automatically learns to describe the content of images by attending to salient parts of an image.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.1502.03044',
    source: 'seed',
  },
  {
    id: 'sohl2015',
    paperId: 'sohl2015',
    title: 'Deep Unsupervised Learning using Nonequilibrium Thermodynamics',
    authors: ['Jascha Sohl-Dickstein', 'Eric A. Weiss', 'Niru Maheswaranathan', 'Surya Ganguli'],
    year: 2015,
    citationCount: 5000,
    abstract: 'We develop a deep generative model using diffusion probabilistic models inspired by non-equilibrium statistical physics. The original diffusion paper.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.1503.03585',
    source: 'seed',
  },
  {
    id: 'song2021',
    paperId: 'song2021',
    title: 'Score-Based Generative Modeling through Stochastic Differential Equations',
    authors: ['Yang Song', 'Jascha Sohl-Dickstein', 'Diederik P. Kingma', 'Abhishek Kumar', 'Stefano Ermon', 'Ben Poole'],
    year: 2021,
    citationCount: 6000,
    abstract: 'We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise. Score SDE.',
    fieldsOfStudy: ['Computer Science'],
    doi: '10.48550/arXiv.2011.13456',
    source: 'seed',
  },
];

// Citation relationships: [citing paper id, cited paper id]
const SEED_CITATIONS = [
  // Transformer lineage
  ['devlin2019', 'vaswani2017'],
  ['brown2020', 'vaswani2017'],
  ['brown2020', 'radford2019'],
  ['brown2020', 'devlin2019'],
  ['openai2023', 'brown2020'],
  ['openai2023', 'vaswani2017'],
  ['openai2023', 'ouyang2022'],
  ['openai2023', 'wei2022'],
  ['radford2019', 'vaswani2017'],
  ['raffel2020', 'vaswani2017'],
  ['raffel2020', 'devlin2019'],
  ['liu2019', 'devlin2019'],
  ['liu2019', 'vaswani2017'],
  ['touvron2023', 'vaswani2017'],
  ['touvron2023', 'brown2020'],
  ['touvron2023', 'ouyang2022'],
  ['chowdhery2022', 'vaswani2017'],
  ['chowdhery2022', 'brown2020'],
  ['chowdhery2022', 'devlin2019'],
  ['ouyang2022', 'brown2020'],
  ['ouyang2022', 'schulman2017'],
  ['wei2022', 'brown2020'],
  ['wei2022', 'chowdhery2022'],
  ['bubeck2023', 'openai2023'],
  ['bubeck2023', 'wei2022'],
  ['kaplan2020', 'vaswani2017'],
  ['kaplan2020', 'radford2019'],
  ['lewis2020', 'devlin2019'],
  ['lewis2020', 'vaswani2017'],

  // Attention mechanism lineage
  ['vaswani2017', 'bahdanau2015'],
  ['vaswani2017', 'sutskever2014'],
  ['bahdanau2015', 'sutskever2014'],
  ['bahdanau2015', 'hochreiter1997'],
  ['sutskever2014', 'hochreiter1997'],
  ['xu2015', 'bahdanau2015'],

  // CNN / Vision lineage
  ['he2016', 'krizhevsky2012'],
  ['he2016', 'simonyan2015'],
  ['he2016', 'szegedy2015'],
  ['he2016', 'ioffe2015'],
  ['simonyan2015', 'krizhevsky2012'],
  ['szegedy2015', 'krizhevsky2012'],
  ['ronneberger2015', 'krizhevsky2012'],
  ['redmon2016', 'krizhevsky2012'],
  ['redmon2016', 'he2016'],
  ['redmon2016', 'szegedy2015'],
  ['ren2015', 'krizhevsky2012'],
  ['ren2015', 'simonyan2015'],
  ['dosovitskiy2021', 'vaswani2017'],
  ['dosovitskiy2021', 'he2016'],
  ['radford2021', 'vaswani2017'],
  ['radford2021', 'dosovitskiy2021'],
  ['radford2021', 'he2016'],

  // GAN / Generative lineage
  ['goodfellow2014', 'kingma2014'],
  ['kingma2014', 'hinton2006'],
  ['rombach2022', 'ho2020'],
  ['rombach2022', 'goodfellow2014'],
  ['rombach2022', 'radford2021'],
  ['ho2020', 'sohl2015'],
  ['song2021', 'ho2020'],
  ['song2021', 'sohl2015'],
  ['ramesh2022', 'radford2021'],
  ['ramesh2022', 'ho2020'],
  ['ramesh2022', 'rombach2022'],

  // RL lineage
  ['silver2016', 'mnih2015'],
  ['silver2017', 'silver2016'],
  ['silver2017', 'mnih2015'],
  ['schulman2017', 'mnih2015'],

  // GNN lineage
  ['hamilton2017', 'kipf2017'],
  ['velickovic2018', 'kipf2017'],
  ['velickovic2018', 'hamilton2017'],
  ['velickovic2018', 'bahdanau2015'],

  // Optimization / foundations
  ['he2016', 'kingma2015'],
  ['devlin2019', 'kingma2015'],
  ['srivastava2014', 'krizhevsky2012'],
  ['srivastava2014', 'hinton2006'],
  ['ioffe2015', 'krizhevsky2012'],
  ['hendrycks2016', 'srivastava2014'],
  ['loshchilov2019', 'kingma2015'],

  // Word embeddings
  ['devlin2019', 'mikolov2013'],
  ['bahdanau2015', 'mikolov2013'],
  ['radford2019', 'mikolov2013'],

  // NAS
  ['zoph2017', 'he2016'],
  ['zoph2017', 'schulman2017'],
];

export function getSeedGraphData() {
  const nodes = SEED_PAPERS.map(paper => ({
    ...paper,
    val: Math.max(3, Math.log10(paper.citationCount + 1) * 3),
  }));

  const links = SEED_CITATIONS.map(([source, target]) => ({
    source,
    target,
  }));

  return { nodes, links };
}

export { SEED_PAPERS, SEED_CITATIONS };
